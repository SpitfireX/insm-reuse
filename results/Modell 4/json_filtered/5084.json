{
  "a_id": "EqmXHTenGFGdsSx3mrHOIenpvuM=",
  "a_publication": "insm_presse",
  "a_date": "2018-09-13T00:00:00",
  "a_author": null,
  "a_url": "https://www.insm.de/insm/presse/pressemeldungen/pressemitteilung-marktwirtschaftlicher-zukunftsdialog",
  "a_text": "Pressemitteilung Marktwirtschaftlicher Zukunftsdialog\nPressemeldung als PDFINSM-Zukunftsdialog Deutschland gibt mehr als die Hälfte seines Bundeshaushalts für Soziales aus. Anfang der 90er Jahre war es nur etwas mehr als ein Drittel. Der Anteil am Gesamtbudget ist damit inzwischen deutlich höher als in fast allen anderen Staaten der OECD. Die große Frage ist: ist die Zusammensetzung des Budgets ausreichend zukunftsorientiert? Darüber debattierten heute Vormittag in Berlin Ralph Brinkhaus MdB (stellvertretender Vorsitzender der CDU/CSU-Bundestagsfraktion) und Anja Hajduk MdB (stellvertretende Fraktionsvorsitzende Bündnis 90/Die Grünen). In einem Impulsvortrag erläuterte Prof. Niklas Potrafke (ifo Institut) die Ergebnisse seiner Studie zur langfristigen Entwicklung der Budgetzusammensetzung. Prof. Potrafke stellte dar, wie sehr gegenwärtig Ausgaben für Soziale Sicherung den Staatshaushalt dominieren. „Der demografische Wandel ist eine tickende Zeitbombe auch für die öffentlichen Budgets, an deren Ticken wir uns nicht gewöhnen dürfen.“ Ralph Brinkhaus ermahnte, dass sich die politische Debatte zu sehr um Probleme von Randgruppen drehe: „Es ist ein Versäumnis der Politik der vergangenen Jahre, sich nicht ausreichend um die Mitte der Gesellschaft zu kümmern. Wir müssen in den Zusammenhalt der Gesellschaft investieren.“ Grundsätzlich appellierte er dafür, den politischen Prozesses der Haushaltsaufstellung neu zu gestalten. „Wir sollten unsere Haushaltspolitik modernisieren und die Budgets stärker zielorientiert aufstellen und dann prüfen, ob unsere Ziele auch erreicht werden“, so Brinkhaus. Dass die bisherige Form der Haushaltsaufstellung überarbeitet werden muss, meint auch Anja Hajduk: „Wir müssen klarer inhaltlich rechtfertigen, wofür wir das Geld ausgeben wollen und besser überprüfen, ob wir die erhofften Ziele auch erreicht haben.“ Mit Blick auf die aktuelle Haushaltsdebatte mahnte Hajduk mehr Weitsicht an: „Die Gefahr ist sehr groß, sich an eine positive Haushaltsentwicklung zu gewöhnen. Es ist geradezu fahrlässig, jetzt in der Rentenpolitik mit der Gießkanne zusätzliche Mittel zu verteilen die in Zukunft kaum zu finanzieren sein werden und obendrein nicht einmal zielgerichtet gegen Altersarmut wirken.“ Hubertus Pellengahr, Geschäftsführer der INSM, erinnerte daran, dass die zusätzlichen Rentenausgaben zu Lasten der nächsten Generationen kein Wohlstandswachstum schaffen würden. „Die Zukunft unseres Landes hängt nicht maßgeblich vom Rentenniveau ab, sondern viel entscheidender vom Bildungsniveau oder dem Niveau der Infrastruktur. Nur wenn wir zu allererst die Wettbewerbsfähigkeit Deutschlands sichern, werden wir uns unsere Sozialausgaben auch in Zukunft leisten können. Das sollte der Maßstab nachhaltiger Haushaltspolitik sein.“ Die Veranstaltung wurde moderiert von Martin Greive (Handelsblatt). Pressesprecher Tel.: 030-27877 174 Fax.: 030-27877 181 E-Mail: hennet@insm.de Twitter: @INSMPresse",
  "b_id": "F5DobhkKUdUCw/vRFsfgKYL/GOU=",
  "b_publication": "SZ",
  "b_date": "2021-04-19T00:00:00",
  "b_author": "Andrian Kreye",
  "b_url": "https://www.sueddeutsche.de/kultur/coded-bias-netflix-doku-1.5268189",
  "b_text": "Film \"Coded Bias\" auf Netflix: Code und Vorurteil\nUm die Erkenntnis aus dem Dokumentarfilm \"Coded Bias\" mal ganz unkorrekt zu verallgemeinern: Der Algorithmus an sich ist ein Rassist. Ein Frauen-, Klassen- und Menschenfeind ist er auch noch. Und wer hat das der Welt schon wieder eingebrockt? Reiche, weiße Männer. Viele von ihnen sind schon tot. Manche werden mithilfe ihrer Algorithmen stündlich reicher. Das ist zwar nur eine plumpe Vereinfachung der Vereinfachung, aber genau diese scharfe Fokussierung auf die sozialen und menschlichen Folgen von Entscheidungsprozessen, die mithilfe mathematischer Handlungsanweisungen und Wahrscheinlichkeitsmodelle automatisiert wurden, ist die eigentliche Qualität des Films, den die Regisseurin Shalini Kantayya in den USA, England und China gedreht hat. \"Coded Bias\" (\"Vorprogrammierte Diskriminierung\") ist nach \"The Great Hack\" (\"Cambridge Analyticas großer Hack\") und \"The Social Dilemma\" (\"Das Dilemma mit den sozialen Medien\") die dritte Dokumentation in Spielfilmlänge, die Netflix zum Thema digitale Technologien und deren Auswirkungen auf die wirkliche Welt in den letzten zwei Jahren veröffentlicht hat. \"Coded Bias\" ist der bisher beste Film. Zum einen, weil er seine Sozialkritik fundiert mit Wissenschaft untermauert. Zum anderen liefert er den bisher klarsten Blick auf ein Themenfeld, das für große Teile der Öffentlichkeit so komplex wie langweilig ist. Was zunächst einmal mit der sehr technischen Kausalkette der digitalen Diskriminierung zu tun hat. Ein verzerrter Datensatz ist nicht nur ein technisches Problem, sondern ein gesellschaftliches Die beginnt mit den Grundzügen des Maschinenlernens. Das funktioniert in den meisten Fällen immer noch so, dass eine künstliche Intelligenz (KI) mit einer größtmöglichen Datenmenge gefüttert wird. Aus diesen \"Big Data\" erschließt sich die KI mithilfe von Wahrscheinlichkeitsrechnungen ihre Resultate. Shalini Kantayya erzählt die Geschichte mit einem dramaturgischen Kniff - wie die meisten zeitgenössischen Fernsehfilme und -serien - von hinten. Sie beginnt mit der in Kanada geborenen ghanaisch-US-amerikanischen Computerwissenschaftlerin Joy Buolamwini, die am Media Lab des Massachusetts Institute of Technology arbeitet, jenem Institut, das seit den Achtzigerjahren viel Pionierarbeit bei den Beziehungen zwischen Mensch und Maschine geleistet hat. Die entdeckte bei der Arbeit an einem Kunstprojekt, dass eine Software, die ihr Gesicht vermessen sollte, um andere Gesichter darüber zu projizieren, nicht so recht funktionierte. Wenn sie allerdings eine weiße Plastikmaske über ihr Gesicht schob, funktionierte die Software hervorragend. Auf der Suche nach der Fehlerquelle kam sie bald darauf, dass das keine Fehlfunktionen der Kamera oder der Software waren, sondern ein Problem in der Tiefe der Daten, mit der das System trainiert wurde. Denn die KI des Programms wurde fast ausschließlich mit Bilddateien weißer Gesichter gefüttert. Bei dunkler Hautfarbe erkannte der Rechner deswegen kein Gesicht. Der sogenannte \"bias\" des Datensatzes führte in Joy Buolamwinis Projekt nicht nur zu einem falschen, sondern zu gar keinem Ergebnis. \"Bias\" ist nicht nur das englische Wort für Vorurteil und Tendenz, sondern auch der technische Begriff für die Verzerrung eines Datensatzes in eine bestimmte Richtung durch falsche Parameter. Die Mathematik definiert den Begriff als systematischen Fehler einer Schätzfunktion. Weil Schätzfunktionen seit der Marktreife Künstlicher-Intelligenz-Anwendungen allerdings unzählige Entscheidungen liefern, die Auswirkungen auf die echte Welt und vor allem auf Menschenleben haben, ist \"bias\" ein Problemfeld mit gesellschaftlicher Sprengkraft. Denn ein verzerrter Datensatz ist nicht nur ein technisches Problem. \"Man kann das Gesellschaftliche nicht vom Technischen trennen\", heißt es im Film. Die KI-Forscherin Meredith Broussard von der New York University erklärt das so: \"Rechner fällen keine ethischen, sondern mathematische Entscheidungen. Und sie richten sich nach der Welt, die sie da draußen vorfinden.\" Die Welt der technischen Institute und Tech-Firmen aber ist eine andere als die für den Rest der Bürger: \"Weniger als 14 Prozent all jener, die künstliche Intelligenz entwickeln, sind Frauen.\" Für den Rest der Welt kommt erschwerend hinzu, dass sich die Entwicklung von KI-Technologie derzeit im Wesentlichen auf neun Konzerne konzentriert. Sechs davon sind aus den USA, drei davon aus China. Beide Länder aber haben im Umgang mit digitalen Technologien gezeigt, dass das Gemeinwohl oder gar ethische Werte kaum oder gar keine Rolle spielen. Shalini Kantayya präsentiert in ihrem Film viele der wichtigsten Technologiekritikerinnen. Virginia Eubanks, Autorin von \"Automating Inequality\". Cathy O'Neil, Autorin von \"Weapons of Math Destruction\". Timnit Gebru, KI-Forscherin, die bis vergangenen Dezember die Abteilung für Ethik in der KI bei Google leitete, bevor sie unter unrühmlich dubiosen Umständen gefeuert wurde. Die Tatsache, dass ihre Kronzeuginnen alle Frauen und zum Großteil nicht weiße Frauen sind, ist weniger eigener \"bias\" als dem Umstand geschuldet, dass Frauen und nicht Weiße die Folgen digitaler Diskriminierung als Erste zu spüren bekamen und sich deswegen auch als Erste wissenschaftlich damit auseinandersetzten. Der Film zeigt nicht nur die destruktive, sondern auch die verführerische Wirkung der Überwachung \"Coded Bias\" ist aber kein reiner Talking-Heads-Film. Er ist ein Feuerwerk aufschlussreicher Anekdoten. Das mag ein dramaturgisches Manko sein, schafft aber meist Klarheit. In London begleitet Kantayya etwa die Aktivisten von \"Big Brother Watch UK\". England ist nach China das Land mit dem durchdringendsten Überwachungssystem. Da wird vor Kantayyas laufender Kamera ein dunkelhäutiger 14-jähriger Schüler verhaftet, weil ihn eine Überwachungskamera fälschlicherweise als Kriminellen identifiziert. Es stellt sich rasch heraus, dass er unschuldig ist. Fingerabdrücke muss er trotzdem abgeben. Ein Mann, der sich vor der Überwachungskamera einen Rollkragen übers Gesicht zieht, wird festgehalten und bekommt eine Geldstrafe. In Brooklyn wiederum begleitet die Regisseurin Bewohnerinnen eines riesigen Wohnblocks, die sich gegen die Überwachungskameras auf den Gängen wehren, weil ihnen die Vermieter Abmahnungen schicken, wenn sie sich auf den Gängen nicht so benehmen, wie die sich das vorstellen. Shalini Kantayya zeigt aber nicht nur die destruktive, sondern auch die verführerische Wirkung der Überwachungs-KIs. Sie begleitet beispielsweise die Studentin Wei Su durch den Alltag, die auf einem Skateboard durch die chinesische Stadt Hangzhou rollert, im Supermarkt Obst einkauft, mit der Bahn fährt, durch eine Mall schlendert, eine Dose aus dem Getränkeautomaten zieht. Überall wird sie registriert und bewertet. Überall kann sie sich aber auch mit ihrem Gesicht einloggen und bezahlen. Wei Su sieht das nicht kritisch. Im Gegenteil. Der Bedienungskomfort der allgegenwärtigen Gesichtserkennungen ist durchaus überzeugend. Selbst das Punktesystem, mit dem die Partei Bürger bewertet und Anrechte auf Bildung, Wohnung, Beruf und sogar Verkehr zuteilt, findet sie in Ordnung. \"Ich denke, dass ein soziales Kreditsystem das Verhalten der Menschen verbessern wird\", sagt sie. \"Gesichtserkennung und Kreditsystem ergänzen sich gegenseitig. Man will sich gut benehmen, weil das Gesicht die Kreditwürdigkeit repräsentiert.\" China ist immer wieder das eindrücklichste Beispiel für die Probleme der digitalen Überwachung und Bewertung. Die Futuristin Amy Webb warnt vor der Selbstgefälligkeit westlicher Demokraten: \"Wenn wir uns das Überwachungs- und Punktesystem Chinas ansehen, sagen viele Leute, Gott sei Dank leben wir nicht dort. In Wirklichkeit werden wir die ganze Zeit bewertet. Wir alle haben täglich mit dem algorithmischen Determinismus zu kämpfen. Irgendein Algorithmus hat Ihnen irgendwo einen Wert zugewiesen, und als Ergebnis zahlen Sie mehr oder weniger Geld für Toilettenpapier, wenn Sie online einkaufen. Es werden Ihnen bessere oder schlechtere Hypotheken angezeigt. Es ist wahrscheinlicher oder unwahrscheinlicher, dass Sie als Krimineller profiliert werden. Der entscheidende Unterschied zwischen den Vereinigten Staaten und China ist, dass China das transparent macht.\" Wie die meisten Dokus auf Netflix leidet auch \"Coded Bias\" in Europa darunter, dass der Blickwinkel ein sehr amerikanischer ist. In der Technologiedebatte spielt das allerdings nur eine untergeordnete Rolle. Europa bleibt eine digitale Kolonie der USA. Auch in Deutschland sind die wichtigsten Anwendungen Produkte amerikanischer Konzerne wie das iPhone, Facebook, Amazon oder Google. Was den Film für ein breites Publikum so wertvoll macht, ist die Tatsache, dass digitaler Fortschritt bei Marktreife immer sofort als Erleichterung des Alltags und Komfort erfahrbar ist. Dinge gehen schneller, einfacher, billiger. Die gesellschaftlichen Auswirkungen verbreiten sich viel schleichender. Wenn sie aber spürbar sind, ist es oft zu spät. Der Film greift da früh in die Debatte ein. In einer Zeit, in der die Pandemie große Teile der Arbeitswelt und das gesamte Gesundheitssystem in die Digitalisierung zwingen, zu einem Zeitpunkt also, an dem in Höchstgeschwindigkeit die digitale Infrastruktur der Zukunft aufgebaut wird, ist die Verzerrung der Datensätze kein banales Problem. Diese Maschinen replizieren zum einen den Status quo einer Gesellschaft und bremsen so jeden sozialen Fortschritt. Aber nicht nur das. Durch die Vervielfachung von Vorurteilen in den Datensätzen besteht sogar die Gefahr, dass sozialer Fortschritt wieder rückgängig gemacht wird. Und das ist für alle ein Problem. Nur für die reichen, weißen Männer nicht. Coded Bias - Vorprogrammierte Diskriminierung. USA 2020. Regie und Buch: Shalini Kantayya. Kamera: Steve Acevedo. Schnitt: Alexandra Gilwit. Mit: Joy Buolamwini, Meredith Broussard, Amy Webb, Cathy O'Neil, Virginia Eubanks, Timnit Gebru. Auf Netflix. 85 Minuten.",
  "scores": "{\"1\": 0.13333333333333333, \"2\": 0.029567854435178165, \"3\": 0.004490057729313663, \"4\": 0.0006226650062266501, \"5\": 0.0, \"hapax\": 0.06367924528301887, \"1_mod\": 1.0, \"2_mod\": 0.1263537906137184, \"3_mod\": 0.008616047388260635, \"4_mod\": 0.0005841121495327102, \"5_mod\": 0.0}"
}